\documentclass[12pt]{article}
\usepackage[numbers]{natbib}
\usepackage[margin=1in]{geometry}
\usepackage[nottoc]{tocbibind}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{setspace}

\graphicspath{{../images}}

\author{Thomas Kwashnak}
\title{DS-480 Project}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\begin{quote}
	(Rest of document is in progress, discussion is being submitted)
\end{quote}

\section{Abstract}

\section{Introduction}


% Board Ideas

\paragraph{}
Reinforcement learning is a subset of machine learning that aims to complete a goal in an environment.
This can range from solving a maze, to playing a video game, to learning how to drive a car.
Regardless of the environment, the goal of the reinforcement learning agent is to find a policy, or a mapping of all possible states to the actions the agent takes, that will maximize some long-term reward \cite{article_reinforcement_learning_survey}.
An agent does this by learning the Q-Values associated with the environment, or the estimated reward for taking some action $A$ at a state $S$.
This method alone is infeasable for more complicated environments, where accounting for every action in every possible state is nearly impossible to represent, or would take too long to train.
Deep-Q Learning provides a solution by using a neural network to \textit{estimate} the Q-Values of each action given the current state \cite{article_human_level_control_deep_reinforcement_learning}.

% Remaining Issues
\paragraph{}
Simple back propagation techniques, used to train and modify neural networks, can often run into the issue of requiring a lot of time to train \cite{article_accelerating_neural_networks_weight_extrapolations}.
As the data sets we use get bigger and more complex, the longer it takes for models to train and accurately model the patterns presented.
Deep-Q learning models are no different.
Additionally, some environments may have many states with no reward and few states with positive or negative reward.
This means that agents must take more steps before finding a positive or negative reward, and can lead agents to mindlessly exploring before finding any reward \cite{article_approx_optimal_approximate_reinforcement_learning}.

% Project Details

\paragraph{}
This paper explores a potential training improvement for deep reinforcement learning models in environments with large state space and a known goal.
The idea is that an agent learning to back-track its own steps is faster than exploring the whole environment.
First, the agent will be given a smaller state-space closer to the end-goal, and then expanded over time to allow the agent to learn the entire maze.
The agent will be trained alongsize control agent to compare the ability each agent has to solve the maze over their training time.
The data collected will be able to suggest whether this training strategy improves the training rate of the model.

\section{Methods}


\subsection{Environment}
\paragraph{}

In this project, the task of the agent is to explore a simple maze.
The maze is stored as a 2-dimensional array of locations, which can either be empty or filled in.
The maze itself is generated and stored using mazelib \cite{lib_mazelib} in Python \cite{lang_python}.
The output of the environment consists of the maze configuration, the current location, and the target goal.
The maze configuration is represented by a flattened array representing empty ($0$) spaces and walls ($1$).
The current position and goal position are stored as their x and y coordinates using $n * m$ variables.
Similar to the configuration, 


\subsection{Reinforcement Learning Model}

\subsection{Agent Training}

\subsection{Data Collection}


\section{Results (WIP)}


NOTE: This section is WIP. To work on the Discussion page, I'm building an outline (using the data presented in the poster) to drive my discussion.

\begin{itemize}
	\item Both models tended to go all-or-nothing with one particular action
	\item Over time, the models swapped the preferred action from two of the four possible actions. (Left and Right)
\end{itemize}


% talk about how the model typically started picking one move at a time after the first target update
% How it only picked 2 moves to switch between


\begin{figure}[h]
	\includesvg[width=\linewidth]{svg-graphs/graph_evaluation}
    \caption{Model evaluation over time. Evaluation is calculated from the policy generated by the agent, represented as the percent of the maze that the policy successfully reaches the end from. Higher is Better.}
		\label{fig:evaluation}
\end{figure}

\begin{figure}[h]
	\includesvg[width=\linewidth]{svg-graphs/graph_directional}
	\caption{Policy frequency of actions over time... TODO}
	\label{fig:directional}
\end{figure}


\begin{figure}[h]
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_up}
			\subcaption{Iteration 350}
			\label{fig:policy_350}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_mixed}
			\subcaption{Iteration 400}
			\label{fig:policy_400}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 450}
			\label{fig:policy_450}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 500}
			\label{fig:policy_500}
		\end{subfigure}
	\end{center}
	\caption{Mixed Policy of Experimental Model... TODO}
	\label{fig:policy}
\end{figure}


% \subsection{Previous Results Draft}
%
% \paragraph{} 
% The two models were trained on identical datasets.
% In total, $10,000$ observations were recorded for each model. 
% Each model was given the same observations and dataset, and trained in the same manner.
%
%
% \paragraph{}
% Figure \ref{fig:graphs} shows the model evaluation throughout the training.
% The control model did not change it's policy at all, so it maintained a consistent Evaluation level.
% The experimental model has two distinct properties.
% First, it has signicicantly lower evaluations than the control model.
% At no point during the training did the experimental model obtain a higher evaluation than the control model.
% Second, the model shows an initial dip in it's evaluation during the first $500$ iterations.
% Afterwards, it slowly climbs back up, but never higher than it had in the first few iterations.
%

\newpage
(blank for results)
\newpage
(blank for results)
\newpage

\section{Discussion}

% Overview of the Results
% - Discussing how the model liked to pick one particular move
% - That particular move tended to switch up between one of two moves
% 
% Interpretation of the results
% - Model must have had high expected utilities for the move that got the agent to the solution in the beginning
% - Even when the target updated, the model still believed that taking that move would result in higher expected utility
% - 


\paragraph{}
The results show that neither model ended up learning how to solve the maze. 
While Figure \ref{fig:evaluation} both models appear to be getting worse over time.
However, the results from Figure \ref{fig:evaluation} are complicated at best.
Since the evaluation is calculated as percent of moves that will take the agent to the goal by following the agent's policy, simply changing the policy for the state directly next to the goal turns any progress immediately to 0.
Therefore, this method of evaluating the model is not necessarily the best at measuring the effectiveness of a policy.

\paragraph{}
One of the main points that the results show is that the models liked to prefer one particular action at a time regardless of the current state.
As seen in Figure \ref{fig:directional}, both models often seemed to rotate its preferred action between two of the four actions. % TODO: include this in results
One of the causes for this could be some form of overfitting.
Prior research indicates that when trained on one particular environment, a reinforcement learning model will overfit to that particular environment, and not be applicable to other environments \cite{article_overfitting_neural_networks}.
Similarly if the model is only trained on a specific subset of the environment, we can reason that it will overfit on that area.
This is a noticeable problem in the experimental model, since we start its training with a small state-space around the goal.
The model will first learn that only one action is correct, the action that takes it to the goal.
When the model is then taken outside of that area, it may assume that its still near the goal, and just take that action.
This implies that the experimental method could in fact be detrimental to the convergence rate of the model.
When a model starts out overfitting on the goal region, it can take significantly longer to overcome that.

\paragraph{}
However, one of the things we notice in the results is that the model tends to switch up the favorite action.
For example, Figure \ref{fig:policy} shows the transition from one favorite action to another in the policy.
At first, the entire policy is to always choose the up direction.
By the end of the change, the entire policy is to always choose the right direction.
An interesting note is that if you take every optimal step towards the goal (such as in a perfect solution), every state that is within 4-5 actions away from the goal is either up or right.
This implies that the agent is simply picking one of the valid actions that actually leads it towards the goal.
Once again, this can be seen as a result of overfitting on one particular action.
After overfitting on one action, one of the only ways that the agent can pick another action is if it starts only choosing one of the other actions.
Typically, that action also results in the agent getting closer to the goal.

\paragraph{}
There are several things that can be improved in this project in future iterations.
Firstly, the usefuleness of the current evaluation isn't the best at measuring the effectiveness of the policy.
A better policy could be something akin to comparing the current policy with the optimal policy.
The optimal policy could either be a static policy, or the policy that the agent decides is the best at the end.
Next, the experimental method should theoretically work in a more simpler model.
Using a Q-table instead of a neural network would remove a lot of the randomization and provide more consistent results.
By using a Q-table, a lot of the overfitting issues would be somewhat removed, and the policy is more localized to each individual state's position.
Lastly, there are several ways to set up the reinforcement learning agent.
Since this project was under a short time constraint, there was little room to experiment and try out different methods for training cycles, target updates, etc.
If more time was given, these parameters and methods could be optimized to train both networks in the least number of iterations possible, allowing the data to show the difference in convergence rates.


% Bring up sources in introduction**

% 15-20 pages (not including title page or referenes)
% OH BOY
% Include code in project
% GitHub Link 


% For example, the experimental state is first fed states right next to the goal, making the replay database consist of just one state.
% This leads the model to learn that taking one particular action results in obtaining the max reward for being in the goal.
% However, as the model is given newer states slightly further from the goal, the replay network is still filled with a significant portion of that one state.
% As this continues, the expected utility of taking that action is stuck in a feedback loop that keeps increasing it.
% After some point, the replay is filled with another specific action that also, for some reason, has a high expected utility.












\newpage
\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}

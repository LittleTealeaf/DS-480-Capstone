\documentclass[12pt,letterpaper]{article}
\usepackage[numbers]{natbib}
\usepackage[margin=1in]{geometry}
\usepackage[nottoc]{tocbibind}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{setspace}
\usepackage{amsmath}

\doublespacing

\graphicspath{{../images}}

\author{Thomas Kwashnak}
\title{Supervising Deep Q-Learning in known Environments}

\begin{document}

\maketitle

\newpage

\section{Abstract}

Deep Q-Learning utilizes the unique properties of neural networks to apply Q-Learning to larger environments.
However, one of the drawbacks of using neural networks is that a significant amount of data is required for the model to learn.
This paper attempts to explore modifications that could be made to the reinforcement learning method in environments with known goals.
The modification involves starting the agent closer to the goal and slowly expanding its knowledge of the environment.
While no definitive results were drawn, several insights are gained, such as the importance of avoiding letting the agent overfit on environments.

\section{Introduction}

Reinforcement learning is a subset of machine learning where an agent learns to maximize a reward in an environment \cite{article_reinforcement_learning_survey}.
These can range from solving simple mazes and puzzles, to complex real world environments such as driving a car \cite{article_deep_learning_hybrid_quantum}.
Environments consist of states, or points in time observed by the agent, and actions that the agent performs that takes it to the next state.
The goal of the agent is to observe the current state and select an action that maximizes its long term reward.
Agents do this by using a Q-Function, which converts a state-action pair into the expected future reward that we call the Q-Value.
In simpler implementations, this is done with a simple table-map, where the key is the state-action pair and the value is the Q-Value.
During training, the Q-Values are incrementally updated until it accurately models the environment.
A policy, or the mapping from states to selected actions, can then be created from the Q-Function by visiting every state and selecting the action with the highest Q-Value.

Not all environments can simply list out all possible states, which is a critical requirement when implementing the Q-Function with a table.
That is, it's infeasable to visit each state and pick out the best action to take.
In order to use reinforcement learning to solve these types of environments, neural networks are used to approximate the Q-Function \cite{article_reinforcement_learning_for_robots}.
This is called Deep Q-Learing, as it requires a large amount of data in order to generalize the results to best approximate the Q-Function, alternatively called the Q-Network.
Whereas using a table allows weights to be updated for individual states and actions as the model progresses, the Q-Network needs to generalize the patterns of the whole environment.
Thus, Deep Q-Learning often takes a significant amount of time training in order to optimize the agent to maximize long term reward.

This paper explores a potential improvement to training Deep Q-Learning models in environments that have a known end goal.
This includes environments such as mazes or map navigation, where the goal is clear but the optimal path to reach it may not be.
Research has been done to explore kickstarting reinforcement learning agents with teacher agents \cite{article_kickstart_deep_reinforcement_learning}, taking a larger scale approach to modifying the learning method.
The method presented in this paper instead seeks to change how the training data itself is generated.
It starts by training the agent right next to the goal, so the model understands where it needs to go.
Then, as the training continues, the agent is taken further and further away from the goal, giving enough time so that it can navigate its way back to the goal.
The idea is that by showing the agent the goal first, it avoids needing to initially explore to find the goal.
In this paper, two agents are trained side by side on the same maze.
One represents the standard implementation of reinforcement learning, while the other trains using this modified technique.
The models are then analyzed by looking at their policy functions over time, taking note the progress each makes towards solving the maze.

\section{Methods}

\paragraph{Environment}

The goal of this project is to focus on the training method, not necessarily to care about the problem it's solving.
Thus, any environment that fulfils the requirements will suffice.
For this project, the environment chosen was to explore and solve a simple maze.

The environment is built around mazes generated using the Python \cite{lang_python} llibrary MazeLib \cite{lib_mazelib}.
Mazes are created using MazeLib's Prims algorithm.
MazeLib stores mazes as a 2 dimensional array of 1s and 0s.
1s indicate walls and 0s indicates empty spaces that the agent can navigate through.
Additionally, MazeLib generates a start and exit square.
The exit square is used as the goal, and the start square is used as the start.

The environment wrapper class is built to provide helper functions for use by the agent.
First, it store the current location of the agent.
It also provides methods for the agent to use to move, which will not allow the agent to move if it attempts to move into a wall.
At every state, the agent can pick one of four moves: up, down, left, or right.
Up reduces the $y$ coordinate by 1, Down increases the $y$ coordinate by 1, Right increases the $x$ coordinate by 1, and Left decreases the $x$ coordinate by 1.
Lastly, it provides a method to convert the current state into a vector in order to feed into the neural network.

In order to keep training even simpler, the same maze is used throughout the entire process.
Because we aren't trying to make the agent solve any maze, we don't need to necessarily provide the current setup of the maze.
The source code, however, does implement methods to include the current state if needed.
Therefore, the only information the agent needs to know is the ccurrent location.
The location is provided as several variables, repeated for both the $x$ and $y$ coordinates.
Since the dimensions of the maze are known, and in this instances relatively small, the coordinate is treated as a categorical variable.
That is, there are $h + w$ values in the output tensor, where $h$ is the height and $w$ is the width.
For example, the dimensions used for the maze in this paper is $9x9$.
Thus, the form is as follows. The variables are equal to $1$ if the condition is true, and $0$ otherwise.

$$\left [ x = 1 , x = 2 , x = 3 , x = 4 , \hdots , y = 5 , y = 6 , y = 7 , y = 8 \right]$$

Say the model is at the point ($3$, $5$), the vector would be as follows.

$$\left[0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0\right]$$

While the position could be conveyed in less variables, making the position into a categorical-like variable helps the neural network easily differentiate between the positions.

In order to teach the agent where the goal is, the environment needs a reward function that relates the current state to some corresponding immediate reward.
A simple implementation of this is to set the reward to 1 if the agent is at the goal, and 0 otherwise.
However, to give the agent some better context to know when it's getting better, similar to the warmer-colder game, a scaling reward also works.
The reward used in this paper scales from $1$ when it's at the goal, and $0$ when it's furthest away from the goal.
Specifically, the manhattan distance is compared to the maximum manhattan distance possible in the maze.
The reward uses the following formula, where dist is the current manhattan distance from the agent to the goal, and max dist is the maximum distance possible, which is $\text{height} + \text{width}$.

$$\text{Reward} = \frac{(\text{max dist} - \text{dist}) ^ 2}{\text{max dist}^2}$$

The exponents in this function can be scaled to control how far of a drop off there is as you move away from the goal.
This is done in order to prevent a consistent small increase in reward as the agent steps towards the goal, but rather an exponential increase to incentivize the agent to get towards the goal faster.

\paragraph{Deep Q-Learning Agent}

The agent used in this project implements a basic version of Deep Q-Learning.
Deep Q-Learning is an extension of Q-Learning, which teaches agents how to optimally perform in a markovian decision tree \cite{article_q_learning}.

The difference between Deep Q-Learning and Q-Learning is the use of a neural network to approximate the Q-Function \cite{article_human_level_control_deep_reinforcement_learning}.
The Q-Function's purpose is to take state-action pairs and return the expected utility, or long-term reward, obtained from taking that action from that state.
In order to simplify and remove the need to run the Q-Network for each state-action pairs, we can modify it to accept the current state and have an output node for each action \cite{article_reinforcement_learning_survey}.
Thus, by inputting the current state, the neural network outputs the expected utility of each possible action.
The only caveat with this setup is that the model will provide an expected utility for an action, even if it isn't possible in the environment.
In the environment used for this paper, if the agent tries to make a move that takes it into a wall, the agent just doens't move and it's taken from there.

During the first part of Deep Q-Learning, the agent is placed in the environment.
The agent is given the state it is in, where it is fed into the Q-Network.
By some random $\varepsilon$ chance, the agent either picks the action with the highest predicted expected utility, or selects a random action \cite{article_reinforcement_learning_survey}.
In this paper, $\varepsilon$ was decided by the following equation:

$$\varepsilon = 0.6 * 0.99^{\text{iter} \% 100} + 0.4$$

The goal of this equation is to provide a repeating range of $\varepsilon$ values.
The 100 indicates the frequency that the target network is updated.
That is, when the target network is updated, the $\varepsilon$ value's cycle will reset.

During this phase, the agent gathers the state, action, resulting state, and immediate reward into what's called a replay database \cite{article_reinforcement_learning_for_robots}.
The replay database provides the agent with a form of short-term memory that it trains off of.
The agent uses a random sample of the replay database to update the weights of the Q-Network.
This uses a combination of the Q-Network, and a duplicate network called the target network \cite{article_human_level_control_deep_reinforcement_learning}.
The target network is a network that is less frequently updated, but instead matches a version of the Q-Network in an earlier iteration.
In this paper, the target network is updated with the current values of the Q-Network every 100 iterations.
The target network is used to stabilize the training, so the network isn't training against a constantly updating 'target' \cite{article_human_level_control_deep_reinforcement_learning}.

The goal of training the weights is to match the expected value outputted from the Q-Network to a combination of the immediate reward and future expected reward returned from the target \cite{article_reinforcement_learning_survey}.
That is, the expected utility of the Q-Network for a given state-action pair is compared to the reward of the following state + the maximum expected utility when that state is fed into the target network.
The following equation summarizes what the Q-Network is trying to become:

$$Q(s_{1})_{a} \leftarrow R(s_2) + \gamma \cdot \max_i(T(s_2)_i)$$

In summarization, the evaluation of the Q-network at state  $s_1$ for action $a$ should become the sum of the Reward at state $s_2$ plus $\gamma$ times the maximum expected utility of any action taken from state $s_2$ using the target network.
$\gamma$ is a discount factor used to incentivize the agent to look for paths with less steps \cite{article_reinforcement_learning_for_robots}.
In this paper, a $\gamma$ value of $0.9$ was used, and did not change throughout the training.

The values in the Q-Network were trained using a dynamic learning rate.
The learning rate is used to scale the changes made to the values in the network \cite{article_reinforcement_learning_for_robots}.
In this paper, the learning rate takes on the following function based on the iteration count.

$$\alpha = 0.01 * 0.99^{\text{iter} \% 100}$$

This function, similar to the $\epsilon$ function, scales the learning rate based on how long since the last target update the agent is.
The thought process is that once the target network is updated, the Q-network has a new target to train against, so resetting the learning rate allows it to change to match that target better.

\paragraph{Training} 

The difference between the control agent and the experimental agent is particular to the way that it gathers observations into the replay database.
The control method takes the typical approach; it is placed at the beginning of the maze and explores from there \cite{article_reinforcement_learning_survey}.
This is how it typically is done, as it does not assuming that the goal is known.
Then, as the agent goes through exploitation and exploration phases, it slowly learns the environment until it eventually stumbles on the goal.

The experimental agent takes advantage of the fact that we know where the goal is in the environment.
Thus, the agent can be manually placed in states right next to the goal.
The goal is still the same: use the agent to build a policy that takes the agent from any state to the goal.
During early phases of the training, the agent is only placed right next to the goal.
The agent is reset often to ensure that it does not stray far.
As training progresses, the agent is slowly placed further and further from the goal.
After each time the range increases, enough time is given to allow the agent to learn its way back to the goal.
In this paper, the agent's range was expanded by 1 additional move away from the goal every 200 iterations, or 2 target updates.

\paragraph{Policy Evaluation}

To inspect progress during the training, the a policy was generated from the agent after every iteration.
The policy is created by placing the agent in every valid state within the environment, and taking the maximum output of the Q-Network's evaluation.


\paragraph{Data Collection and Analysis}

In order to provide insight into the convergence rates of both models, data is collected after each training iteration.
After each training cycle, the policy generated and two sets of variables are recorded.
First, the policy evaluation is included as a primary means of tracking the agent's progress.
Second, the frequency of each unique action used in the policy is included to expose trends in the agent's policy.
These data points are then stored in a CSV file after training is completed.
To analyze the data, the CSV file is loaded into R \cite{lang_r} and aggregated using libraries included in the tidyverse \cite{lib_tidyverse} package.
The visualizations shown in this paper are created using ggplot2 \cite{lib_ggplot2}.

\section{Results}

\begin{figure}[h]
	\begin{center}
		\includesvg[width=0.95\linewidth]{svg-graphs/graph_evaluation}
	\end{center}
	\caption{Model evaluation over time. Evaluation is calculated from the policy generated by the agent, represented as the percent of the maze that the policy successfully reaches the end from. Higher is Better.}
	\label{fig:evaluation}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includesvg[width=0.95\linewidth]{svg-graphs/graph_directional}
	\end{center}
	\caption{Policy composition over time. The frequency of each individual action is tracked from the policies generated after each iteration. Higher or lower is not necessarily better, but rather it's expected that the optimal solution has a mixture of all actions depending on the maze itself. The vertical line at Iteration 100 indicates the point when the target network was updated with the Q-Network values.}
	\label{fig:directional}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_up}
			\subcaption{Iteration 350}
			\label{fig:policy_350}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_mixed}
			\subcaption{Iteration 400}
			\label{fig:policy_400}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 450}
			\label{fig:policy_450}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 500}
			\label{fig:policy_500}
		\end{subfigure}
	\end{center}
	\caption{Policy visualization of the agent through a change in its preferred action. Squares represent valid states in the maze. Actions only allow moving from one state to any other directly adjacent squares.}
	\label{fig:policy}
\end{figure}

Neither model substantially learned much during the training observed.
Even when one step away from the goal, the agents had a difficult time learning that moving to the goal gives a high reward.

Figure \ref{fig:evaluation} depicts the evaluation recorded after each iteration for both models.
The graph shows both models following a downward trend in evaluation, indicating that the model performed worse towards the end of training than starting out.
It should be noted that the range of points in the graph are from $0.105$ and $0.120$.
Evaluation ranges from $0.00$ to $1.00$, so the range of $0.105$ and $0.120$ indicates that extremely little changed in the models.
For most iterations during training, both models had an evaluation score of $0.00$.
The values shown on the graph account for both those iterations, as well as the occasional iteration where the policy had a couple tiles where the policy led the agent towards the exit.

The data pertaining to the frequency an action shows up in the policy tells a different story.
Figure \ref{fig:directional} shows the trend of how often each action shows up in the policy.
Both agents have very similar trends in the frequencies of different actions.
Each agent often only selected one action as the preferred action, regardless of the current state.
For example, at Iteration 80, the Control agent's policy stated that the agent should move left in every state, while the Experimental agent's policy stated that the agent should move right.
The main difference between the models are the rate that they swapped the preferred action.
Figure \ref{fig:directional} only shows a snippet to show the pattern, so it doesn't necessarily tell the story of how often these swaps occurred.
The overall frequency of swaps that either network had throughout training was not necessarily significant enough of a difference between the agents.
The only note is that the preferred action swaps more often after the target network is updated, allowing the variables to change significantly.

Figure \ref{fig:policy} shows an example where the agent's policy changes from always going up to always going to the right.
Starting in Figure \ref{fig:policy_350}, which depicts the policy at iteration 350, the agent prefers to always take the up action.
However, in iteration 400, shown in Figure \ref{fig:policy_400}, the policy consists of a seemingly random mix of either taking the up or right action.
Then in iteration 450 and 500, shown in Figures \ref{fig:policy_450} and \ref{fig:policy_500} respectively, the policy prefers the right action.

\section{Discussion}

The results conclude that neither model learned how to solve the maze in any capacity.
There could be several initial factors that may lead to these inconclusive results.
Most obviously, the agents may need significantly longer than 20,000 iterations to learn anything.
The number 20,000 was arbitrarily picked with the hopes that it struck a balance between a large enough training duration to capture the agents learning, while being small enough to be able to run within a reasonable amount of time.
Additionally, it is likely that some key points of reinforcement learning were not correctly implemented.
However, there is enough information that may give us hints as to the effectiveness of using the proposed training method.

The results from Figure \ref{fig:evaluation} can be misleading.
Some of this has to do with the method of generating the evaluation scores.
In this paper, a breadth-first search algorithm was implemented to start at the goal and find all tiles that the policy would lead to the goal.
While this does work effectively, it doesn't account for changes behind an incorrect move.
Take Figure \ref{fig:policy_400} for example.
The evaluation function would indicate that exaclty one tile will lead the agent to the goal when following the policy, which is the tile right next to the goal.
Thus, the evaluation would be $\frac{1}{31}$, or roughly $0.032$.
However, let's say that we change every value, except for the top row, to have the correct action choice.
Even though the policy itself is still technically the same, the resulting evaluation is still $0.032$, since it only sees the one state that moves the agent into the goal.
A more realistic evaluation would be comparing the agent's current policy to some known optimal policy.
The only issue is that this method of evaluating the convergence of the agent to the optimal policy requires that we \textit{know} the optimal policy.
Thus, it can only really apply towards environments that are used for testing, and can't necessarily be used in evaluating the training of a model without a known optimal policy.
While this doesn't indicate whether or not the experimental method of training a Deep Q-Learning model, it does indicate that the results, mainly in Figure \ref{fig:evaluation}, do not tell the full story of the acccuracy of the model.

One of the biggest key points in both Figure \ref{fig:directional} and Figure \ref{fig:policy} was the tendency for the agent to prefer one action regardless of position.
Additionally, throughout training the model tends to switch between the different actions, never stabilizing on a policy with a combination of actions.
This seems to indicate that some form of overfitting is presenting itself in the model.
That is, the weights corresponding to one action in the output nodes of the Q-Network may have significantly higher values than the other output nodes, drowning out the current state's effect on the output.
Previous research has found that reinforcement learning agents that are trained on only one environment will overfit to that environment, making several assumptions that may not carry over to other environments \cite{article_overfitting_neural_networks}.
This means that agents trained in one environment will perform significantly worse in a different environment that may have a different structure or setup.
Take the example of this paper and mazes. In order to better analyze the changes in policy, a single maze was created for the agent to explore.
However, placing the agent in a different maze with the exact same setup will perform signficantly worse, as the agent would not understand that where there might be walls in one maze there are now paths the agent needs to take.

Similarly, this may be the case of the experiment training method.
First, the agent is given a sub-space of the large environment.
That is, the agent is only given states closer to the goals.
The agent then might over-fit on those states, which may initially make one direction significantly more appealing than the rest.
Therefore, the weights associated with that move are increasingly higher than the other actions.
Then, as the model is brought out to new states in the environment, it may still believe that it's in that smaller sub-space of the whole environment.
The issue is that the actions preferred within the sub-space have significantly higher weights than the other actions, making it hard to change the policy for only one state without changing the rest of the states.
This challenges the idea that the model would converge faster to the optimal strategy when shown the goal first.
When the model overfits near the goal, it has a significantly harder time working with the rest of the environment.
However, the question still remains on the root cause of this observation, since this overfitting seems to be affecting both the experiemntal method and the control method, which doesn't have the same explanation for its overfitting.
It could be several factors, ranging from the design of the neural network itself, to the configuration of learning rates and training sizes.
Regardless, it brings up a question about how to navigate the overfitting problem with an agent starting within a smaller sub-space of the environment.

There are some potential solutions that could be implemented to navigate this potential issue.
In Deep Q-Learning, the goal of the Q-Network is to approximate the function that converts state-action pairs to expected utilities \cite{article_reinforcement_learning_survey}.
Intermittently, the target network is updated with the weights of the Q-Network.
The goal of the target network is to store the expected utilities of looking one more step ahead.
When the target network is updated, the weights and biases in the Q-Network don't matter as much, since they need to be trained against a new target.
Therefore, we can modify the network to something of our choosing right after updating the target network.
This could either be by squishing all of the weights and biases to be within a given range, or by providing a completely new randomly generated network.
However, this adds some randomness to the training method, so there is a chance that the agent takes steps backwards during training.

\newpage
\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}

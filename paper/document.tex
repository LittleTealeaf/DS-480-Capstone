\documentclass[12pt]{article}
\usepackage[numbers]{natbib}
\usepackage[margin=1in]{geometry}
\usepackage[nottoc]{tocbibind}

\author{Thomas Kwashnak}
\title{DS-480 Project}

\begin{document}

\maketitle

\newpage

\section{Abstract}

\section{Introduction}


% Board Ideas

\paragraph{}

Reinforcement learning is a subset of machine learning that aims to complete a goal in an environment.
This can range from solving a maze, to playing a video game, to learning how to drive a car.
Regardless of the environment, the goal of the reinforcement learning agent is to find a policy, or a mapping of all possible states to the actions the agent takes, that will maximize some long-term reward \cite{article_reinforcement_learning_survey}.
An agent does this by learning the Q-Values associated with the environment, or the estimated reward for taking some action $A$ at a state $S$.
This method alone is infeasable for more complicated environments, where accounting for every move in every possible state is nearly impossible to represent, or would take too long to train.
Deep-Q Learning provides a solution by using a neural network to \textit{estimate} the Q-Values of each action given the current state \cite{article_human_level_control_deep_reinforcement_learning}.


% Remaining Issues
\paragraph{}
Simple back propagation techniques, used to train and modify neural networks, can often run into the issue of requiring a lot of time to train \cite{article_accelerating_neural_networks_weight_extrapolations}.
As the data sets we use get bigger and more complex, the longer it takes for models to train and accurately model the patterns presented.
Deep-Q learning models are no different.
Additionally, some environments may have many states with no reward and few states with positive or negative reward.
This means that agents must take more steps before finding a positive or negative reward, and can lead agents to mindlessly exploring before finding any reward \cite{article_approx_optimal_approximate_reinforcement_learning}.

% Project Details

\paragraph{}
This paper explores a potential training improvement for deep reinforcement learning models in environments with large state space and a known goal.
The idea is that an agent learning to back-track its own steps is faster than exploring the whole environment.
First, the agent will be given a smaller state-space closer to the end-goal, and then expanded over time to allow the agent to learn the entire maze.
The agent will be trained alongsize control agent to compare the ability each agent has to solve the maze over their training time.
The data collected will be able to suggest whether this training strategy improves the training rate of the model.


\section{Methods}

\subsection{Environment}

\paragraph{} 
The environment that the agents explore is a simple maze.
The maze consists of a 2-dimensional array of empty spaces and walls.
The environment output given to the agent contains both the maze and the agent's position.
The maze is encoded in $0$s and $1$s, representing empty spaces and walls respectively.
This is given for every tile on the map.
The agent's position will be given by its X and Y coordinates, encoded into a binary format.
% Alternatively this may be encoded as 00100 00010 etc.
The agent is placed at a random point in the maze, and its goal is to reach the top left corner $(0,0)$

\subsection{Reinforcement Learning Model}

\paragraph{}
The agent will be implemented and trained based on a simple Deep-Q machine learning model \cite{article_reinforcement_learning_survey}.

\subsection{Agent Specifications}

\subsection{Data Collection and Analysis}

\paragraph{}

\section{Discussion}

\newpage
\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}

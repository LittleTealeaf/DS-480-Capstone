\documentclass[12pt]{article}
\usepackage[numbers]{natbib}
\usepackage[margin=1in]{geometry}
\usepackage[nottoc]{tocbibind}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{setspace}
\usepackage{amsmath}

\doublespacing

\graphicspath{{../images}}

\author{Thomas Kwashnak}
\title{DS-480 Project}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Abstract}

\section{Introduction}

% \paragraph{}
% Reinforcement learning is a subset of machine learning that aims to complete a goal in an environment.
% This can range from solving a maze, to playing a video game, to learning how to drive a car.
% Regardless of the environment, the goal of the reinforcement learning agent is to find a policy, or a mapping of all possible states to the actions the agent takes, that will maximize some long-term reward \cite{article_reinforcement_learning_survey}.
% An agent does this by learning the Q-Values associated with the environment, or the estimated reward for taking some action $A$ at a state $S$.
% This method alone is infeasable for more complicated environments, where accounting for every action in every possible state is nearly impossible to represent, or would take too long to train.
% Deep-Q Learning provides a solution by using a neural network to \textit{estimate} the Q-Values of each action given the current state \cite{article_human_level_control_deep_reinforcement_learning}.
%
% % Remaining Issues
% \paragraph{}
% Simple back propagation techniques, used to train and modify neural networks, can often run into the issue of requiring a lot of time to train \cite{article_accelerating_neural_networks_weight_extrapolations}.
% As the data sets we use get bigger and more complex, the longer it takes for models to train and accurately model the patterns presented.
% Deep-Q learning models are no different.
% Additionally, some environments may have many states with no reward and few states with positive or negative reward.
% This means that agents must take more steps before finding a positive or negative reward, and can lead agents to mindlessly exploring before finding any reward \cite{article_approx_optimal_approximate_reinforcement_learning}.
%

Reinforcement learning is a subset of machine learning where an agent learns to maximize a reward in an environment \cite{article_reinforcement_learning_survey}.
These can range from solving simple mazes and puzzles, to complex real world environments such as driving a car.
Environments consist of states, or points in time observed by the agent, and actions that the agent performs that takes it to the next state.
The goal of the agent is to observe the current state and select an action that maximizes its long term reward.
Agents do this by using a Q-Function, which converts a state-action pair into the expected future reward that we call the Q-Value.
In simpler implementations, this is done with a simple table-map, where the key is the state-action pair and the value is the Q-Value.
During training, the Q-Values are incrementally updated until it accurately models the environment.
A policy, or the mapping from states to selected actions, can then be created from the Q-Function by visiting every state and selecting the action with the highest Q-Value.

Not all environments can simply list out all possible states, which is a critical requirement when implementing the Q-Function with a table.
That is, it's infeasable to visit each state and pick out the best action to take.
In order to use reinforcement learning to solve these types of environments, neural networks are used to approximate the Q-Function \cite{article_reinforcement_learning_for_robots}.
This is called Deep Q-Learing, as it requires a large amount of data in order to generalize the results to best approximate the Q-Function, alternatively called the Q-Network.
Whereas using a table allows weights to be updated for individual states and actions as the model progresses, the Q-Network needs to generalize the patterns of the whole environment.
Thus, Deep Q-Learning often takes a significant amount of time training in order to optimize the agent to maximize long term reward.

This paper explores a potential improvement to training Deep Q-Learning models in environments that have a known end goal.
This includes environments such as mazes or map navigation, where the goal is clear but the optimal path to reach it may not be.
The proposed method is to start training the agent right next to the goal, so the model understands where it needs to go.
Then, as the training continues, the agent is taken further and further away from the goal, giving enough time so that it can navigate its way back to the goal.
The idea is that by showing the agent the goal first, it can 



% % Project Details
%
% \paragraph{}
% This paper explores a potential training improvement for deep reinforcement learning models in environments with large state space and a known goal.
% The idea is that an agent learning to back-track its own steps is faster than exploring the whole environment.
% First, the agent will be given a smaller state-space closer to the end-goal, and then expanded over time to allow the agent to learn the entire maze.
% The agent will be trained alongsize control agent to compare the ability each agent has to solve the maze over their training time.
% The data collected will be able to suggest whether this training strategy improves the training rate of the model.

\section{Methods}


\paragraph{Environment}

In this project, the task of the agent is to explore a simple maze.
The maze is stored in a 2-dimensional array consisting of empty spaces and walls, generated using the Python \cite{lang_python} library MazeLib \cite{lib_mazelib}.
A wrapper class is created to manage the initial randomization seed, current location, reward function, and observational output.

Throughout training, the same maze is used to keep it simpler.
For this reason, the state vector used as input for the model only needs to describe the agent's current location.
This will be done by providing both the $x$ and $y$ coordinate of the agent with $n + m$ variables, where $n$ is the width and $m$ is the height.
The location will be represented by setting the corresponding $x$ and $y$ variables to $1$, and leaving the rest as $0$.
For example, the dimensions of the maze used in this paper is $9x9$.
Thus, the state vector consists of $18$ variables; $9$ to specify the $x$ position, and $9$ to specify the $y$ position.

In order to direct the agent towards the goal, a reward function is used that associates the current position with some reward value.
For this project, the reward function will scale based on how far away the agent is from the goal.
When the agent is at the goal, it will obtain a reward of $1$. When it is further away from the goal, it receives a reward close to $0$.
The implementation used for this project uses the manhattan distance to calculate the reward.
It uses the following formula, where $\text{dist}$ is the manhattan distance from the agent to the goal, and $\text{max dist}$ is the maximum distance possible in the maze, or the maze's $\text{height} + \text{width}$.

$$\text{Reward} = \frac{(\text{max dist} - \text{dist}) ^ 2}{\text{max dist}^2}$$

This reward function results in an exponential drop in reward when the agent moves away from the goal, but keeps the reward inbetween the bounds of $0$ to $1$.


\paragraph{Agent}
The agent used in this project is a very simple implementation of a Deep Q-Learning agent.
Deep Q-Learning is an extension of Q-Learning, which teaches agents how to optimally perfor in a Markovian Decision Tree \cite{article_q_learning}.
Q-Learning uses a Q-Function, typically as a table, to track the expected utilities of being in some state and taking some action.
For example, the function or table would return a high expected utility if the state was right next to the goal and the action took the agent to that goal.

In Q-Learning, the agent explores the environment and gathers observations of states and actions into a replay experience \cite{article_reinforcement_learning_for_robots}.
This replay experience is then used to train and update the expected utility for the given state-action pairs.
In doing this, the model will over time be able to accurately predict the expected utility of every state-action pair.
The policy can then be created by going through each state and picking the action with the highest expected utility from that state.

Deep Q-Learning extends Q-Learning by using a neural network to approximate the Q-Function \cite{article_human_level_control_deep_reinforcement_learning}.
The neural network used in such a way is called the DQN, or Deep Q-Network.
To reduce the number of times it needs to be run, the DQN accepts the current state as the input, and a distinct output for each of the possible actions \cite{article_reinforcement_learning_survey}.
The goal of training the DQN is to make each of the distinct outputs equal the expected utility of taking that action from the inputted state.

In order to improve the stability of training, a target network is used to estimate the future reward \cite{article_human_level_control_deep_reinforcement_learning}.
The target network is simply a copy of the DQN, but it only gets updated after a certain number of training cycles.
This is done so that the DQN isn't trying to train against a constantly updating version of itself, but rather a previous version of itself.

The agent is implemented in Python \cite{lang_python}, and uses the TensorFlow \cite{lib_tensorflow} library to accelerate performance using an NVIDIA GPU.

\paragraph{Training} 
The difference between the control model and the experimental model is down to the particular way of providing entries into the replay database.
When the control model starts training, it's placed at the natural beginning of the environment, as it would if it were in many other common environments like video games \cite{article_reinforcement_learning_survey}.
This means that the agent needs to first explore in order to find the goal, slowly learning the environment.

The experimental model takes a different approach and starts close to the goal.
At first, the agent is placed right next to the goal, where it can easily figure out where it needs to go.
Throughout training, the agent is slowly placed further and further away from the goal.
This is done slow enough so the agent can reacquaint itself and remember where the goal is.


\paragraph{Policy Evaluation}

Throughout training, some evaluation of how well the model can solve the maze is needed to track progress.
To do this, the policy is first created from the model by placing the agent in every possible state and recording the action with the highest expected utility \cite{article_reinforcement_learning_survey}.
Then, we start at the goal and perform a breadth first search to find the number of states that, by following the policy, will lead to the goal.
We divide the number of states that lead to the goal in this way by the total number of states to obtain the proportion of states that lead to the goal.
This value can be used as a measurement of how much of the maze the agent can solve using its policy.

\paragraph{Data Collection and Analysis}

In order to provide insight into the convergence rates of both models, data is collected after each training iteration.
After each training cycle, the policy generated and two sets of variables are recorded.
First, the policy evaluation is included as a primary means of tracking the agent's progress.
Second, the frequency of each unique action used in the policy is included to expose trends in the agent's policy.
These data points are then stored in a CSV file after training is completed.
To analyze the data, the CSV file is loaded into R \cite{lang_r} and aggregated using libraries included in the tidyverse \cite{lib_tidyverse} package.
The visualizations shown in this paper are created using ggplot2 \cite{lib_ggplot2}.

\section{Results (WIP)}


NOTE: This section is WIP. To work on the Discussion page, I'm building an outline (using the data presented in the poster) to drive my discussion.

\begin{itemize}
	\item Both models tended to go all-or-nothing with one particular action
	\item Over time, the models swapped the preferred action from two of the four possible actions. (Left and Right)
\end{itemize}


% talk about how the model typically started picking one move at a time after the first target update
% How it only picked 2 moves to switch between


\begin{figure}[h]
	\includesvg[width=\linewidth]{svg-graphs/graph_evaluation}
    \caption{Model evaluation over time. Evaluation is calculated from the policy generated by the agent, represented as the percent of the maze that the policy successfully reaches the end from. Higher is Better.}
		\label{fig:evaluation}
\end{figure}

\begin{figure}[h]
	\includesvg[width=\linewidth]{svg-graphs/graph_directional}
	\caption{Policy frequency of actions over time... TODO}
	\label{fig:directional}
\end{figure}


\begin{figure}[h]
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_up}
			\subcaption{Iteration 350}
			\label{fig:policy_350}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_mixed}
			\subcaption{Iteration 400}
			\label{fig:policy_400}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 450}
			\label{fig:policy_450}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 500}
			\label{fig:policy_500}
		\end{subfigure}
	\end{center}
	\caption{Mixed Policy of Experimental Model... TODO}
	\label{fig:policy}
\end{figure}


% \subsection{Previous Results Draft}
%
% \paragraph{} 
% The two models were trained on identical datasets.
% In total, $10,000$ observations were recorded for each model. 
% Each model was given the same observations and dataset, and trained in the same manner.
%
%
% \paragraph{}
% Figure \ref{fig:graphs} shows the model evaluation throughout the training.
% The control model did not change it's policy at all, so it maintained a consistent Evaluation level.
% The experimental model has two distinct properties.
% First, it has signicicantly lower evaluations than the control model.
% At no point during the training did the experimental model obtain a higher evaluation than the control model.
% Second, the model shows an initial dip in it's evaluation during the first $500$ iterations.
% Afterwards, it slowly climbs back up, but never higher than it had in the first few iterations.
%

\newpage
(blank for results)
\newpage
(blank for results)
\newpage

\section{Discussion}

% Overview of the Results
% - Discussing how the model liked to pick one particular move
% - That particular move tended to switch up between one of two moves
% 
% Interpretation of the results
% - Model must have had high expected utilities for the move that got the agent to the solution in the beginning
% - Even when the target updated, the model still believed that taking that move would result in higher expected utility
% - 


\paragraph{}
The results show that neither model ended up learning how to solve the maze. 
While Figure \ref{fig:evaluation} both models appear to be getting worse over time.
However, the results from Figure \ref{fig:evaluation} are complicated at best.
Since the evaluation is calculated as percent of moves that will take the agent to the goal by following the agent's policy, simply changing the policy for the state directly next to the goal turns any progress immediately to 0.
Therefore, this method of evaluating the model is not necessarily the best at measuring the effectiveness of a policy.

\paragraph{}
One of the main points that the results show is that the models liked to prefer one particular action at a time regardless of the current state.
As seen in Figure \ref{fig:directional}, both models often seemed to rotate its preferred action between two of the four actions. % TODO: include this in results
One of the causes for this could be some form of overfitting.
Prior research indicates that when trained on one particular environment, a reinforcement learning model will overfit to that particular environment, and not be applicable to other environments \cite{article_overfitting_neural_networks}.
Similarly if the model is only trained on a specific subset of the environment, we can reason that it will overfit on that area.
This is a noticeable problem in the experimental model, since we start its training with a small state-space around the goal.
The model will first learn that only one action is correct, the action that takes it to the goal.
When the model is then taken outside of that area, it may assume that its still near the goal, and just take that action.
This implies that the experimental method could in fact be detrimental to the convergence rate of the model.
When a model starts out overfitting on the goal region, it can take significantly longer to overcome that.

\paragraph{}
However, one of the things we notice in the results is that the model tends to switch up the favorite action.
For example, Figure \ref{fig:policy} shows the transition from one favorite action to another in the policy.
At first, the entire policy is to always choose the up direction.
By the end of the change, the entire policy is to always choose the right direction.
An interesting note is that if you take every optimal step towards the goal (such as in a perfect solution), every state that is within 4-5 actions away from the goal is either up or right.
This implies that the agent is simply picking one of the valid actions that actually leads it towards the goal.
Once again, this can be seen as a result of overfitting on one particular action.
After overfitting on one action, one of the only ways that the agent can pick another action is if it starts only choosing one of the other actions.
Typically, that action also results in the agent getting closer to the goal.

\paragraph{}
There are several things that can be improved in this project in future iterations.
Firstly, the usefuleness of the current evaluation isn't the best at measuring the effectiveness of the policy.
A better policy could be something akin to comparing the current policy with the optimal policy.
The optimal policy could either be a static policy, or the policy that the agent decides is the best at the end.
Next, the experimental method should theoretically work in a more simpler model.
Using a Q-table instead of a neural network would remove a lot of the randomization and provide more consistent results.
By using a Q-table, a lot of the overfitting issues would be somewhat removed, and the policy is more localized to each individual state's position.
Lastly, there are several ways to set up the reinforcement learning agent.
Since this project was under a short time constraint, there was little room to experiment and try out different methods for training cycles, target updates, etc.
If more time was given, these parameters and methods could be optimized to train both networks in the least number of iterations possible, allowing the data to show the difference in convergence rates.


% Bring up sources in introduction**

% 15-20 pages (not including title page or referenes)
% OH BOY
% Include code in project
% GitHub Link 


% For example, the experimental state is first fed states right next to the goal, making the replay database consist of just one state.
% This leads the model to learn that taking one particular action results in obtaining the max reward for being in the goal.
% However, as the model is given newer states slightly further from the goal, the replay network is still filled with a significant portion of that one state.
% As this continues, the expected utility of taking that action is stuck in a feedback loop that keeps increasing it.
% After some point, the replay is filled with another specific action that also, for some reason, has a high expected utility.












\newpage
\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}

\documentclass[12pt,letterpaper]{article}
\usepackage[numbers]{natbib}
\usepackage[margin=1in]{geometry}
\usepackage[nottoc]{tocbibind}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{setspace}
\usepackage{amsmath}

\doublespacing

\graphicspath{{../images}}

\author{Thomas Kwashnak}
\title{Supervising Deep Q-Learning in known Environments}

\begin{document}

\maketitle

\newpage

\section{Abstract}

Reinforcement learning, a subset of machine learning, aims to train an agent to perform some long-term goal in an environment.
The basic implementation of Q-Learning works well for small environments, but once the environment size gets larger, it can quickly become infeasable to apply.
Thus, Deep Q-Learning uses neural networks to approximate methods witin Q-Learning, allowing the ability for much more complex environments.
However, Deep Q-Learning requires significantly more data and time in order to train.
In this paper, a modified reinforcement learning method is presented as a potential way to expidite training in environments that follow specific assumptions.
In simple terms, the process starts the agent's training next to the goal to give it immediate reward, and slowly moves it back out to introduce it to the rest of the environment.
The results presented in this paper are not conclusive, and can neither state that the method is better or worse.
However, some insights into the collected data indicate that a form of overfitting is showing up, potentially caused by the proposed method.
This paper concludes that additional research and experimentation with simpler models might be able to demonstrate the effectiveness of this method better.

\section{Introduction}

Reinforcement learning is a subset of machine learning where an agent learns to maximize a reward in some environment \cite{article_reinforcement_learning_survey}.
These can range from solving simple mazes and puzzles, to complex real world environments such as driving a car or controlling a robotic arm through complex tasks \cite{article_deep_learning_hybrid_quantum}.
The idea of reinforcement learning is nothing new, as popularity started to rise back in the 1990s among the machine learning and artificial intelligence communities \cite{article_reinforcement_learning_survey}.
However, reinforcement learning is still ever-growing, as much of it is limited by the technology and capabilities of modern machines to run these algorithms.

Leslie Kaelbling defines reinforcement learning as the method of an agent learning behaviors through trial and error \cite{article_reinforcement_learning_survey}.
The agent interacts with the environment by observing the state through some perception, and specifies the action that the agent wishes to take.
During each interaction, the agent reads the environment through the percieved values.
Then, the agent either explores by selecting a random action, or exploits its knowledge by picking the action it likes the best.
After taking that action, the agent receives some scalar reward to reward good behavior and punish bad behavior.
Once enough interactions happen, the agent is then able to review the actions it took and learn from them.
Kaelbling discusses several different strategies that can be used to implement reinforcement learning, one of the simpler to implement being Q-Learning.
Q-Learning consists of a Q-Function, which accepts a state and an action and returns a Q-Value, or the expected future reward for taking that action from that state.
The Q-Value is typically a discounted expected reward, in order to incentivize shorter solutions in an environment where multiple solutions are possible.
The goal during training is to train the agent to accurately predict the Q-Value.
This paper takes the basics presented in this survey as the baseline for the agents.

Kick-starting reinforcement learning agents allows for better data-efficiency and convergence time \cite{article_kickstart_deep_reinforcement_learning}.
This is done through the use of a `parent' agent, which the student is then trained against to improve its own reward functions.
By using knowledge gained from one or multiple teacher agents, a student agent is able to maximize expected reward significantly faster.
However, this method only considers reinforcement learning situations where pre-trained models already exist.
This assumption is not one that is assumed to be true in the research done in this paper.
Therefore, the use of kick-starting algorithms has little to no use in this paper.
However, insights gained from this paper may influence the ability to create `teacher' agents.

The issue of overfitting found in many other forms of statistical models also apply to neural networks \cite{article_overfitting_neural_networks}.
Research into the potential of overfitting with neural networks show that the ability to overfit is drastically different than other models.
Neural networks are far more complex, and thus have several other potential areas of overfitting.
Even better learning algorithms, such as conjugate gradient, were found to have worse abilities to generalize patterns.
In Deep Q-Learning, the ability to overfit on the data brings up several questions about how to set up the process.
For example, if an agent is presented with the same set of data repeatedly, it will tend to be more accurate with that data versus the states it's rarely been in.
In this paper, we are not trying to build a model that generalizes against all mazes.
However, it is important to understand that overfitting is a significant issue that needs to be addressed in agents that are intended to solve a more complicated problem.

A paper by Volodymyr Mnih that looks into more human-level control successfully attempts to create a model that is generalized to multiple different video games at the same time \cite{article_human_level_control_deep_reinforcement_learning}.
The paper takes a large convolutional neural network to accept the generalized inputs of the screen, and output the direct inputs to the control interface.
They found that the model significantly out-performed previous algorithms, nearing the performance level of a professiional human.
The significance of the research in this paper is that it demonstrates the ability for Deep Q-Learning agents to generalize over multiple different environments at the same time, even if those environments have different rulesets.
This is significant for this paper because the experimental methods described in this paper start training the model on a smaller sub-sample of the environment state-space.
Mnih's paper indicates that even when the agent is trained on an isolated sample space at the same time, it is still possible for the agent to remember previous smaller sample spaces, such as other games, while learning new patterns in newly introduced sub-spaces.

This paper seeks to explore the possibility of improving convergence rate in an environment that is known.
There are two assumptions that this paper requires of the environment for this experimental methods to work.
First, this process assumes that the agent can be manually placed in any state of the environment.
This means that the state cannot be too complex, as the process must be able to place the agent in an arbitrary state as a starting point.
Second, there must be known end-goals that the agent must be trying to navigate towards.
For example, the goal at the end of the maze.
Note that this assumption does not indicate that there is exactly one end-goal of the environment, but rather that the goals must be known.
In this paper, a single goal state is used to simplify the process.
If these two assumptions are met, then we can use it to apply the process presented in this paper.

The method introduced in this paper attempts to cut down on the amount of initial exploring that the agent needs to do.
When an agent has no knowledge of where the goal is, it can take a significant amount of time exploring to find it \cite{article_kickstart_deep_reinforcement_learning}.
This is even more prominent in situations where the reward function is very binary, only giving positive rewards right at the end.
Thus, no immedite feedback can be given to the agent until it finds the goal.
In this new method, the agent is first shown the area immediately around the goal(s).
The idea behind this is that the agent gets an immediate understanding of what `success' looks like.
Then, as the agent is drawn back away from the goal, it is incentivised to find that `success' again, since it now knows that it exists.
This paper compares this method of training against one that takes the default approach of starting the agent at the beginning of the maze.

\section{Methods}

\paragraph{Environment}

The goal of this project is to focus on the training method, not necessarily to care about the problem it's solving.
Thus, any environment that fulfils the requirements will suffice.
For this project, the environment chosen was to explore and solve a simple maze.

The environment is built around mazes generated using the Python \cite{lang_python} llibrary MazeLib \cite{lib_mazelib}.
Mazes are created using MazeLib's Prims algorithm.
MazeLib stores mazes as a 2 dimensional array of 1s and 0s.
1s indicate walls and 0s indicates empty spaces that the agent can navigate through.
Additionally, MazeLib generates a start and exit square.
The exit square is used as the goal, and the start square is used as the start.

The environment wrapper class is built to provide helper functions for use by the agent.
First, it store the current location of the agent.
It also provides methods for the agent to use to move, which will not allow the agent to move if it attempts to move into a wall.
At every state, the agent can pick one of four moves: up, down, left, or right.
Up reduces the $y$ coordinate by 1, Down increases the $y$ coordinate by 1, Right increases the $x$ coordinate by 1, and Left decreases the $x$ coordinate by 1.
Lastly, it provides a method to convert the current state into a vector in order to feed into the neural network.

In order to keep training even simpler, the same maze is used throughout the entire process.
Because we aren't trying to make the agent solve any maze, we don't need to necessarily provide the current setup of the maze.
The source code, however, does implement methods to include the current state if needed.
Therefore, the only information the agent needs to know is the ccurrent location.
The location is provided as several variables, repeated for both the $x$ and $y$ coordinates.
Since the dimensions of the maze are known, and in this instances relatively small, the coordinate is treated as a categorical variable.
That is, there are $h + w$ values in the output tensor, where $h$ is the height and $w$ is the width.
For example, the dimensions used for the maze in this paper is $9x9$.
Thus, the form is as follows. The variables are equal to $1$ if the condition is true, and $0$ otherwise.

$$\left [ x = 1 , x = 2 , x = 3 , x = 4 , \hdots , y = 5 , y = 6 , y = 7 , y = 8 \right]$$

Say the model is at the point ($3$, $5$), the vector would be as follows.

$$\left[0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0\right]$$

While the position could be conveyed in less variables, making the position into a categorical-like variable helps the neural network easily differentiate between the positions.

In order to teach the agent where the goal is, the environment needs a reward function that relates the current state to some corresponding immediate reward.
A simple implementation of this is to set the reward to 1 if the agent is at the goal, and 0 otherwise.
However, to give the agent some better context to know when it's getting better, similar to the warmer-colder game, a scaling reward also works.
The reward used in this paper scales from $1$ when it's at the goal, and $0$ when it's furthest away from the goal.
Specifically, the manhattan distance is compared to the maximum manhattan distance possible in the maze.
The reward uses the following formula, where dist is the current manhattan distance from the agent to the goal, and max dist is the maximum distance possible, which is $\text{height} + \text{width}$.

$$\text{Reward} = \frac{(\text{max dist} - \text{dist}) ^ 2}{\text{max dist}^2}$$

The exponents in this function can be scaled to control how far of a drop off there is as you move away from the goal.
This is done in order to prevent a consistent small increase in reward as the agent steps towards the goal, but rather an exponential increase to incentivize the agent to get towards the goal faster.

\paragraph{Deep Q-Learning Agent}

The agent used in this project implements a basic version of Deep Q-Learning.
Deep Q-Learning is an extension of Q-Learning, which teaches agents how to optimally perform in a markovian decision tree \cite{article_q_learning}.

The difference between Deep Q-Learning and Q-Learning is the use of a neural network to approximate the Q-Function \cite{article_human_level_control_deep_reinforcement_learning}.
The Q-Function's purpose is to take state-action pairs and return the expected utility, or long-term reward, obtained from taking that action from that state.
In order to simplify and remove the need to run the Q-Network for each state-action pairs, we can modify it to accept the current state and have an output node for each action \cite{article_reinforcement_learning_survey}.
Thus, by inputting the current state, the neural network outputs the expected utility of each possible action.
The only caveat with this setup is that the model will provide an expected utility for an action, even if it isn't possible in the environment.
In the environment used for this paper, if the agent tries to make a move that takes it into a wall, the agent just doens't move and it's taken from there.

During the first part of Deep Q-Learning, the agent is placed in the environment.
The agent is given the state it is in, where it is fed into the Q-Network.
With a probability of $\varepsilon$, the agent selects a random action, else it chooses the action with the highest predicted expected utility \cite{article_reinforcement_learning_survey}.
% By some random $\varepsilon$ chance, the agent either picks the action with the highest predicted expected utility, or selects a random action \cite{article_reinforcement_learning_survey}.
In this paper, $\varepsilon$ was decided by the following equation:

$$\varepsilon = 0.6 * 0.99^{\text{iter} \% 100} + 0.4$$

The goal of this equation is to provide a repeating range of $\varepsilon$ values.
The 100 indicates the frequency that the target network is updated.
That is, when the target network is updated, the $\varepsilon$ value's cycle will reset.

During this phase, the agent gathers the state, action, resulting state, and immediate reward into what's called a replay database \cite{article_reinforcement_learning_for_robots}.
The replay database provides the agent with a form of short-term memory that it trains off of.
The agent uses a random sample of the replay database to update the weights of the Q-Network.
This uses a combination of the Q-Network, and a duplicate network called the target network \cite{article_human_level_control_deep_reinforcement_learning}.
The target network is a network that is less frequently updated, but instead matches a version of the Q-Network in an earlier iteration.
In this paper, the target network is updated with the current values of the Q-Network every 100 iterations.
The target network is used to stabilize the training, so the network isn't training against a constantly updating 'target' \cite{article_human_level_control_deep_reinforcement_learning}.

The goal of training the weights is to match the expected value outputted from the Q-Network to a combination of the immediate reward and future expected reward returned from the target \cite{article_reinforcement_learning_survey}.
That is, the expected utility of the Q-Network for a given state-action pair is compared to the reward of the following state + the maximum expected utility when that state is fed into the target network.
The following equation summarizes what the Q-Network is trying to become:

$$Q(s_{1})_{a} \leftarrow R(s_2) + \gamma \cdot \max_i(T(s_2)_i)$$

In summarization, the evaluation of the Q-network at state  $s_1$ for action $a$ should become the sum of the Reward at state $s_2$ plus $\gamma$ times the maximum expected utility of any action taken from state $s_2$ using the target network.
$\gamma$ is a discount factor used to incentivize the agent to look for paths with less steps \cite{article_reinforcement_learning_for_robots}.
In this paper, a $\gamma$ value of $0.9$ was used, and did not change throughout the training.

The values in the Q-Network were trained using a dynamic learning rate.
The learning rate is used to scale the changes made to the values in the network \cite{article_reinforcement_learning_for_robots}.
In this paper, the learning rate takes on the following function based on the iteration count.

$$\alpha = 0.01 * 0.99^{\text{iter} \% 100}$$

This function, similar to the $\epsilon$ function, scales the learning rate based on how long since the last target update the agent is.
The thought process is that once the target network is updated, the Q-network has a new target to train against, so resetting the learning rate allows it to change to match that target better.

\paragraph{Training} 

The difference between the control agent and the experimental agent is particular to the way that it gathers observations into the replay database.
The control method takes the typical approach; it is placed at the beginning of the maze and explores from there \cite{article_reinforcement_learning_survey}.
This is how it typically is done, as it does not assuming that the goal is known.
Then, as the agent goes through exploitation and exploration phases, it slowly learns the environment until it eventually stumbles on the goal.

The experimental agent takes advantage of the fact that we know where the goal is in the environment.
Thus, the agent can be manually placed in states right next to the goal.
The goal is still the same: use the agent to build a policy that takes the agent from any state to the goal.
During early phases of the training, the agent is only placed right next to the goal.
The agent is reset often to ensure that it does not stray far.
As training progresses, the agent is slowly placed further and further from the goal.
After each time the range increases, enough time is given to allow the agent to learn its way back to the goal.
In this paper, the agent's range was expanded by 1 additional move away from the goal every 200 iterations, or 2 target updates.

\paragraph{Policy Evaluation}

To inspect progress during the training, the a policy was generated from the agent after every iteration.
The policy is created by placing the agent in every valid state within the environment, and taking the maximum output of the Q-Network's evaluation.
This can create a policy, much like the ones displayed in Figure \ref{fig:policy}.

To compare different policies, they are evaluated on how effective they are at solving the maze.
The evaluation used in this paper measures the number of states in the maze that, when following the policy, will eventually reach the goal.
This is done by back-tracking from the goal state using a breadth-first-search approach.
For example, the evaluation of Figure \ref{fig:policy_350} would result in an evaluation of $0.0$, since none of the states ever lead to the goal.
Figure \ref{fig:policy_400} changes such that the policy of the state next to the goal directs the agent into the goal.
Since there are no states whose policy points into that tile, this policy would have an evaluation of $\frac{1}{31}$.
In both Figure \ref{fig:policy_450} and \ref{fig:policy_500}, there are now 5 states that, when following the policy, direct the agent into the goal.
Thus, the evaluation of the policy is $\frac{5}{31}$.

This policy evaluation method is used to keep track of and analyze the progress of the agents as they train.
If the agents are able to learn, the evaluations should generally be gradually increasing towards $1.0$, which would indicate that the policy fully solves the maze.



\paragraph{Data Collection and Analysis}
The experiment process was conducted on a machine using TensorFlow \cite{lib_tensorflow} on an NVIDIA RTX 3080 GPU with 10G of VRAM.
Both the control agent and the experimental agent are trained and tested against the same maze over 20,000 iterations each.

In order to provide insight into the convergence rates of both models, data is collected after each training iteration.
After each training cycle, the policy generated and two sets of variables are recorded.
First, the policy evaluation is included as a primary means of tracking the agent's progress.
Second, the frequency of each unique action used in the policy is included to expose trends in the agent's policy.
These data points are then stored in a CSV file after training is completed.
To analyze the data, the CSV file is loaded into R \cite{lang_r} and aggregated using libraries included in the tidyverse \cite{lib_tidyverse} package.
The visualizations shown in this paper are created using ggplot2 \cite{lib_ggplot2}.

\section{Results}

\begin{figure}[h]
	\begin{center}
		\includesvg[width=0.95\linewidth]{svg-graphs/graph_evaluation}
	\end{center}
	\caption{Model evaluation over time. Evaluation is calculated from the policy generated by the agent, represented as the percent of the maze that the policy successfully reaches the end from. Higher is Better.}
	\label{fig:evaluation}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includesvg[width=0.95\linewidth]{svg-graphs/graph_directional}
	\end{center}
	\caption{Policy composition over time. The frequency of each individual action is tracked from the policies generated after each iteration. Higher or lower is not necessarily better, but rather it's expected that the optimal solution has a mixture of all actions depending on the maze itself. The vertical line at Iteration 100 indicates the point when the target network was updated with the Q-Network values.}
	\label{fig:directional}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_up}
			\subcaption{Iteration 350}
			\label{fig:policy_350}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_mixed}
			\subcaption{Iteration 400}
			\label{fig:policy_400}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 450}
			\label{fig:policy_450}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 500}
			\label{fig:policy_500}
		\end{subfigure}
	\end{center}
	\caption{Policy visualization of the agent through a change in its preferred action. Squares represent valid states in the maze. Actions only allow moving from one state to any other directly adjacent squares.}
	\label{fig:policy}
\end{figure}

Neither model substantially learned much during the training observed.
Even when one step away from the goal, the agents had a difficult time learning that moving to the goal gives a high reward.

Figure \ref{fig:evaluation} depicts the evaluation recorded after each iteration for both models.
The graph shows both models following a downward trend in evaluation, indicating that the model performed worse towards the end of training than starting out.
It should be noted that the range of points in the graph are from $0.105$ and $0.120$.
Evaluation ranges from $0.00$ to $1.00$, so the range of $0.105$ and $0.120$ indicates that extremely little changed in the models.
For most iterations during training, both models had an evaluation score of $0.00$.
The values shown on the graph account for both those iterations, as well as the occasional iteration where the policy had a couple tiles where the policy led the agent towards the exit.

The data pertaining to the frequency an action shows up in the policy tells a different story.
Figure \ref{fig:directional} shows the trend of how often each action shows up in the policy.
Both agents have very similar trends in the frequencies of different actions.
Each agent often only selected one action as the preferred action, regardless of the current state.
For example, at Iteration 80, the Control agent's policy stated that the agent should move left in every state, while the Experimental agent's policy stated that the agent should move right.
The main difference between the models are the rate that they swapped the preferred action.
Figure \ref{fig:directional} only shows a snippet to show the pattern, so it doesn't necessarily tell the story of how often these swaps occurred.
The overall frequency of swaps that either network had throughout training was not necessarily significant enough of a difference between the agents.
The only note is that the preferred action swaps more often after the target network is updated, allowing the variables to change significantly.

Figure \ref{fig:policy} shows an example where the agent's policy changes from always going up to always going to the right.
Starting in Figure \ref{fig:policy_350}, which depicts the policy at iteration 350, the agent prefers to always take the up action.
However, in iteration 400, shown in Figure \ref{fig:policy_400}, the policy consists of a seemingly random mix of either taking the up or right action.
Then in iteration 450 and 500, shown in Figures \ref{fig:policy_450} and \ref{fig:policy_500} respectively, the policy prefers the right action.

\section{Discussion}

The results conclude that neither model learned how to solve the maze in any capacity.
There could be several initial factors that may lead to these inconclusive results.
First, and more obviously, the agents may need significantly longer than 20,000 iterations to learn anything.
This is debatable, since results were found in the Kick-Starter paper after only 5,000 iterations \cite{article_kickstart_deep_reinforcement_learning}.
However, by the nature of neural networks, the convergence rate is highly unpredictable based on the complexity of the pattern it needs to approximate.
The number 20,000 was arbitrarily picked with the hopes that it struck a balance between a large enough training duration to capture the agents learning, while being small enough to be able to run within a reasonable amount of time.
Additionally, it is highly likely that some other part of the experiment was flawed, causing some model inefficiencies or outright bugs.
However, there is enough information that may give us hints as to the effectiveness of using the proposed training method.

The results from Figure \ref{fig:evaluation} can be misleading.
Some of this has to do with the method of generating the evaluation scores.
In this paper, a breadth-first search algorithm was implemented to start at the goal and find all states that the policy would lead to the goal.
While this does work effectively, it doesn't account for changes behind an incorrect move.
Take Figure \ref{fig:policy_400} for example.
The evaluation function would indicate that exaclty one tile will lead the agent to the goal when following the policy, which is the tile right next to the goal.
Thus, the evaluation would be $\frac{1}{31}$, or roughly $0.032$.
However, let's say that we change every value, except for the top row, to have the correct action choice that would solve the maze.
Even though the policy itself is technically significantly closer to the desired policy, the resulting evaluation is still $0.032$, since it only sees the one state that moves the agent into the goal.
A more realistic evaluation would be comparing the agent's current policy to some known optimal policy.
The only issue is that this method of evaluating the convergence of the agent to the optimal policy requires that the optimal policy is known.
Thus, it can only really apply towards environments that are used for testing, and can't necessarily be used in evaluating the training of a model without a known optimal policy.
While this doesn't indicate whether or not the experimental method of training a Deep Q-Learning model, it does indicate that the results, mainly in Figure \ref{fig:evaluation}, do not tell the full story of the acccuracy of the model.

One of the biggest key points in both Figure \ref{fig:directional} and Figure \ref{fig:policy} was the tendency for the agent to prefer one action regardless of position.
Throughout training, the model tends to switch between the different actions, never stabilizing on a policy with a combination of actions.
This seems to indicate that some form of overfitting is presenting itself in the model.
That is, the weights corresponding to one action in the output nodes of the Q-Network may have significantly higher values than the other output nodes, drowning out the current state's effect on the output.
In other words, no combination of the inputs allows for other actions to have a higher predicted value than the current preferred aciton.
Previous research has found that reinforcement learning agents that are trained on only one environment will overfit to that environment, making several assumptions that may not carry over to other environments \cite{article_overfitting_neural_networks}.
This means that agents trained in one environment will perform significantly worse in a different environment that may have a different structure or setup.
Take the example of the mazes in this paper. In order to better analyze the changes in policy, a single maze was created for the agent to explore.
However, placing the agent in a different maze with the exact same setup will perform signficantly worse, as the agent would not understand that where there might be walls in one maze there are now paths in the other maze that the agent needs to take.

Similarly, this may be the case within the experiment training method.
First, the agent is given a sub-space of the large environment.
That is, the agent is only given states closer to the goals.
The agent then might over-fit on those states, which may initially make one direction significantly more appealing than the rest.
Therefore, the weights associated with that move are increasingly higher than the other actions.
Then, as the model is brought out to new states in the environment, it may still believe that it's in that smaller sub-space of the whole environment.
The issue is that the actions preferred within the sub-space have significantly higher weights than the other actions, making it hard to change the policy for only one state without changing the rest of the states.
This challenges the idea that the model would converge faster to the optimal strategy when shown the goal first.
When the model overfits near the goal, it has a significantly harder time working in the rest of the environment.
However, the question still remains on the root cause of this observation, since this overfitting seems to be affecting both the experiemntal method and the control method, which doesn't have the same explanation for its overfitting.
It could be several factors, ranging from the design of the neural network itself, to the configuration of learning rates and training sizes.
Regardless, this brings up the question about how to navigate the overfitting problem with an agent starting within a smaller sub-space of the environment.

There are some potential solutions that could be implemented to navigate this potential issue.
When the target network is updated, the weights and biases in the Q-Network don't matter as much, since they need to be trained against a new target.
Therefore, we can modify the network right after updating the target network without drastically changing the result, since it already needs to converge to some new pattern/configuration.
This could either be by squishing all of the weights and biases to be within a given range, or by providing a completely new randomly generated network.
However, this may add some randomness to the training method, so there is a chance that the agent takes steps backwards during training.

Further research could also seek to combine this method with the kick-starting process \cite{article_kickstart_deep_reinforcement_learning}.
That is, a process such as the one presented in this paper could be used as rough `teacher' for a more sophisticated model.
This would mean that the student would be able to observe the teacher's knowledge of where the goal is, and learn from there.
Once again, this would only apply to environments where the assumptions made in this paper hold, but it could drastically improve the ability to quickly build teacher agents.

In conclusion, we cannot say whether or not the method discussed in this paper is effective or ineffective.
Rather, we can note that it is a lot more complicated than it may seem.
Future work could perhaps take some steps back to observe if this method can be used with a more typical Q-Function, such as a simple table.
This would be possible given the small size of the environment used in this paper.
Additionally, perhaps a more complicated environment that still holds the assumptions presented in this paper could be found to give the agent a challenge.
The only limiting factor of this is the duration of time that the agents could be allowed to train and collect data from.

\newpage
\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}

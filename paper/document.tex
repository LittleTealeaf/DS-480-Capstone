\documentclass[12pt]{article}
\usepackage[numbers]{natbib}
\usepackage[margin=1in]{geometry}
\usepackage[nottoc]{tocbibind}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{setspace}
\usepackage{amsmath}

\doublespacing

\graphicspath{{../images}}

\author{Thomas Kwashnak}
\title{DS-480 Project}

\begin{document}

\maketitle

\newpage

\section{Abstract}

\section{Introduction}

Reinforcement learning is a subset of machine learning where an agent learns to maximize a reward in an environment \cite{article_reinforcement_learning_survey}.
These can range from solving simple mazes and puzzles, to complex real world environments such as driving a car \cite{article_deep_learning_hybrid_quantum}.
Environments consist of states, or points in time observed by the agent, and actions that the agent performs that takes it to the next state.
The goal of the agent is to observe the current state and select an action that maximizes its long term reward.
Agents do this by using a Q-Function, which converts a state-action pair into the expected future reward that we call the Q-Value.
In simpler implementations, this is done with a simple table-map, where the key is the state-action pair and the value is the Q-Value.
During training, the Q-Values are incrementally updated until it accurately models the environment.
A policy, or the mapping from states to selected actions, can then be created from the Q-Function by visiting every state and selecting the action with the highest Q-Value.

Not all environments can simply list out all possible states, which is a critical requirement when implementing the Q-Function with a table.
That is, it's infeasable to visit each state and pick out the best action to take.
In order to use reinforcement learning to solve these types of environments, neural networks are used to approximate the Q-Function \cite{article_reinforcement_learning_for_robots}.
This is called Deep Q-Learing, as it requires a large amount of data in order to generalize the results to best approximate the Q-Function, alternatively called the Q-Network.
Whereas using a table allows weights to be updated for individual states and actions as the model progresses, the Q-Network needs to generalize the patterns of the whole environment.
Thus, Deep Q-Learning often takes a significant amount of time training in order to optimize the agent to maximize long term reward.

This paper explores a potential improvement to training Deep Q-Learning models in environments that have a known end goal.
This includes environments such as mazes or map navigation, where the goal is clear but the optimal path to reach it may not be.
Research has been done to explore kickstarting reinforcement learning agents with teacher agents \cite{article_kickstart_deep_reinforcement_learning}, taking a larger scale approach to modifying the learning method.
The method presented in this paper instead seeks to change how the training data itself is generated.
It starts by training the agent right next to the goal, so the model understands where it needs to go.
Then, as the training continues, the agent is taken further and further away from the goal, giving enough time so that it can navigate its way back to the goal.
The idea is that by showing the agent the goal first, it avoids needing to initially explore to find the goal.
In this paper, two agents are trained side by side on the same maze.
One represents the standard implementation of reinforcement learning, while the other trains using this modified technique.
The models are then analyzed by looking at their policy functions over time, taking note the progress each makes towards solving the maze.


\section{Methods}


\paragraph{Environment}

In this project, the task of the agent is to explore a simple maze.
The maze is stored in a 2-dimensional array consisting of empty spaces and walls, generated using the Python \cite{lang_python} library MazeLib \cite{lib_mazelib}.
A wrapper class is created to manage the initial randomization seed, current location, reward function, and observational output.

Throughout training, the same maze is used to keep it simpler.
For this reason, the state vector used as input for the model only needs to describe the agent's current location.
This will be done by providing both the $x$ and $y$ coordinate of the agent with $n + m$ variables, where $n$ is the width and $m$ is the height.
The location will be represented by setting the corresponding $x$ and $y$ variables to $1$, and leaving the rest as $0$.
For example, the dimensions of the maze used in this paper is $9x9$.
Thus, the state vector consists of $18$ variables; $9$ to specify the $x$ position, and $9$ to specify the $y$ position.

In order to direct the agent towards the goal, a reward function is used that associates the current position with some reward value.
For this project, the reward function will scale based on how far away the agent is from the goal.
When the agent is at the goal, it will obtain a reward of $1$. When it is further away from the goal, it receives a reward close to $0$.
The implementation used for this project uses the manhattan distance to calculate the reward.
It uses the following formula, where $\text{dist}$ is the manhattan distance from the agent to the goal, and $\text{max dist}$ is the maximum distance possible in the maze, or the maze's $\text{height} + \text{width}$.

$$\text{Reward} = \frac{(\text{max dist} - \text{dist}) ^ 2}{\text{max dist}^2}$$

This reward function results in an exponential drop in reward when the agent moves away from the goal, but keeps the reward inbetween the bounds of $0$ to $1$.


\paragraph{Agent}
The agent used in this project is a very simple implementation of a Deep Q-Learning agent.
Deep Q-Learning is an extension of Q-Learning, which teaches agents how to optimally perfor in a Markovian Decision Tree \cite{article_q_learning}.
Q-Learning uses a Q-Function, typically as a table, to track the expected utilities of being in some state and taking some action.
For example, the function or table would return a high expected utility if the state was right next to the goal and the action took the agent to that goal.

In Q-Learning, the agent explores the environment and gathers observations of states and actions into a replay experience \cite{article_reinforcement_learning_for_robots}.
This replay experience is then used to train and update the expected utility for the given state-action pairs.
In doing this, the model will over time be able to accurately predict the expected utility of every state-action pair.
The policy can then be created by going through each state and picking the action with the highest expected utility from that state.

Deep Q-Learning extends Q-Learning by using a neural network to approximate the Q-Function \cite{article_human_level_control_deep_reinforcement_learning}.
The neural network used in such a way is called the DQN, or Deep Q-Network.
To reduce the number of times it needs to be run, the DQN accepts the current state as the input, and a distinct output for each of the possible actions \cite{article_reinforcement_learning_survey}.
The goal of training the DQN is to make each of the distinct outputs equal the expected utility of taking that action from the inputted state.

In order to improve the stability of training, a target network is used to estimate the future reward \cite{article_human_level_control_deep_reinforcement_learning}.
The target network is simply a copy of the DQN, but it only gets updated after a certain number of training cycles.
This is done so that the DQN isn't trying to train against a constantly updating version of itself, but rather a previous version of itself.

The agent is implemented in Python \cite{lang_python}, and uses the TensorFlow \cite{lib_tensorflow} library to accelerate performance using an NVIDIA GPU.

% TODO: fix

\paragraph{Training} 
The difference between the control model and the experimental model is down to the particular way of providing entries into the replay database.
When the control model starts training, it's placed at the natural beginning of the environment, as it would if it were in many other common environments like video games \cite{article_reinforcement_learning_survey}.
This means that the agent needs to first explore in order to find the goal, slowly learning the environment.

The experimental model takes a different approach and starts close to the goal.
At first, the agent is placed right next to the goal, where it can easily figure out where it needs to go.
Throughout training, the agent is slowly placed further and further away from the goal.
This is done slow enough so the agent can reacquaint itself and remember where the goal is.


\paragraph{Policy Evaluation}

Throughout training, some evaluation of how well the model can solve the maze is needed to track progress.
To do this, the policy is first created from the model by placing the agent in every possible state and recording the action with the highest expected utility \cite{article_reinforcement_learning_survey}.
Then, we start at the goal and perform a breadth first search to find the number of states that, by following the policy, will lead to the goal.
We divide the number of states that lead to the goal in this way by the total number of states to obtain the proportion of states that lead to the goal.
This value can be used as a measurement of how much of the maze the agent can solve using its policy.

\paragraph{Data Collection and Analysis}

% TODO: include the 20,000 iterations
% change this to be more of a "experimental process"

In order to provide insight into the convergence rates of both models, data is collected after each training iteration.
After each training cycle, the policy generated and two sets of variables are recorded.
First, the policy evaluation is included as a primary means of tracking the agent's progress.
Second, the frequency of each unique action used in the policy is included to expose trends in the agent's policy.
These data points are then stored in a CSV file after training is completed.
To analyze the data, the CSV file is loaded into R \cite{lang_r} and aggregated using libraries included in the tidyverse \cite{lib_tidyverse} package.
The visualizations shown in this paper are created using ggplot2 \cite{lib_ggplot2}.

\section{Results}

\begin{figure}[h]
	\begin{center}
	\includesvg[width=0.95\linewidth]{svg-graphs/graph_evaluation}
	\end{center}
    \caption{Model evaluation over time. Evaluation is calculated from the policy generated by the agent, represented as the percent of the maze that the policy successfully reaches the end from. Higher is Better.}
		\label{fig:evaluation}
\end{figure}

\begin{figure}[h]
	\begin{center}
	\includesvg[width=0.95\linewidth]{svg-graphs/graph_directional}
	\end{center}
	\caption{Policy composition over time. The frequency of each individual action is tracked from the policies generated after each iteration. Higher or lower is not necessarily better, but rather it's expected that the optimal solution has a mixture of all actions depending on the maze itself. The vertical line at Iteration 100 indicates the point when the target network was updated with the Q-Network values.}
	\label{fig:directional}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_up}
			\subcaption{Iteration 350}
			\label{fig:policy_350}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_mixed}
			\subcaption{Iteration 400}
			\label{fig:policy_400}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 450}
			\label{fig:policy_450}
		\end{subfigure}
		\hspace{0.1in}
		\begin{subfigure}{0.4\linewidth}
			\includesvg[width=\linewidth]{svg/policy_all_right}
			\subcaption{Iteration 500}
			\label{fig:policy_500}
		\end{subfigure}
	\end{center}
	\caption{Policy visualization of the agent through a change in its preferred action. Squares represent valid states in the maze. Actions only allow moving from one state to any other directly adjacent squares.}
	\label{fig:policy}
\end{figure}


Neither model substantially learned much during the training observed.
Even when one step away from the goal, the agents had a difficult time learning that moving to the goal gives a high reward.

Figure \ref{fig:evaluation} depicts the evaluation recorded after each iteration for both models.
The graph shows both models following a downward trend in evaluation, indicating that the model performed worse towards the end of training than starting out.
It should be noted that the range of points in the graph are from $0.105$ and $0.120$.
Evaluation ranges from $0.00$ to $1.00$, so the range of $0.105$ and $0.120$ indicates that extremely little changed in the models.
For most iterations during training, both models had an evaluation score of $0.00$.
The values shown on the graph account for both those iterations, as well as the occasional iteration where the policy had a couple tiles where the policy led the agent towards the exit.

The data pertaining to the frequency an action shows up in the policy tells a different story.
Figure \ref{fig:directional} shows the trend of how often each action shows up in the policy.
Both agents have very similar trends in the frequencies of different actions.
Each agent often only selected one action as the preferred action, regardless of the current state.
For example, at Iteration 80, the Control agent's policy stated that the agent should move left in every state, while the Experimental agent's policy stated that the agent should move right.
The main difference between the models are the rate that they swapped the preferred action.
Figure \ref{fig:directional} only shows a snippet to show the pattern, so it doesn't necessarily tell the story of how often these swaps occurred.
The overall frequency of swaps that either network had throughout training was not necessarily significant enough of a difference between the agents.
The only note is that the preferred action swaps more often after the target network is updated, allowing the variables to change significantly.

Figure \ref{fig:policy} shows an example where the agent's policy changes from always going up to always going to the right.
Starting in Figure \ref{fig:policy_350}, which depicts the policy at iteration 350, the agent prefers to always take the up action.
However, in iteration 400, shown in Figure \ref{fig:policy_400}, the policy consists of a seemingly random mix of either taking the up or right action.
Then in iteration 450 and 500, shown in Figures \ref{fig:policy_450} and \ref{fig:policy_500} respectively.



\section{Discussion}

The results conclude that neither model learned how to solve the maze in any capacity.
There could be several initial factors that may lead to these inconclusive results.
Most obviously, the agents may need significantly longer than 20,000 iterations to learn anything.
The number 20,000 was arbitrarily picked with the hopes that it struck a balance between a large enough training duration to capture the agents learning, while being small enough to be able to run within a reasonable amount of time.
Additionally, it is likely that some key points of reinforcement learning were not correctly implemented.
However, there is enough information that may give us hints as to the effectiveness of using the proposed training method.

The results from Figure \ref{fig:evaluation} can be misleading.
Some of this has to do with the method of generating the evaluation scores.
In this paper, a breadth-first search algorithm was implemented to start at the goal and find all tiles that the policy would lead to the goal.
While this does work effectively, it doesn't account for changes behind an incorrect move.
Take Figure \ref{fig:policy_400} for example.
The evaluation function would indicate that exaclty one tile will lead the agent to the goal when following the policy, which is the tile right next to the goal.
Thus, the evaluation would be $\frac{1}{31}$, or roughly $0.032$.
However, let's say that we change every value, except for the top row, to have the correct action choice.
Even though the policy itself is still technically the same, the resulting evaluation is still $0.032$, since it only sees the one state that moves the agent into the goal.
A more realistic evaluation would be comparing the agent's current policy to some known optimal policy.
The only issue is that this method of evaluating the convergence of the agent to the optimal policy requires that we \textit{know} the optimal policy.
Thus, it can only really apply towards environments that are used for testing, and can't necessarily be used in evaluating the training of a model without a known optimal policy.
While this doesn't indicate whether or not the experimental method of training a Deep Q-Learning model, it does indicate that the results, mainly in Figure \ref{fig:evaluation}, do not tell the full story of the acccuracy of the model.

One of the biggest key points in both Figure \ref{fig:directional} and Figure \ref{fig:policy} was the tendency for the agent to prefer one action regardless of position.
Additionally, throughout training the model tends to switch between the different actions, never stabilizing on a policy with a combination of actions.
This seems to indicate that some form of overfitting is presenting itself in the model.
That is, the weights corresponding to one action in the output nodes of the Q-Network may have significantly higher values than the other output nodes, drowning out the current state's effect on the output.
Previous research has found that reinforcement learning agents that are trained on only one environment will overfit to that environment, making several assumptions that may not carry over to other environments \cite{article_overfitting_neural_networks}.
This means that agents trained in one environment will perform significantly worse in a different environment that may have a different structure or setup.
Take the example of this paper and mazes. In order to better analyze the changes in policy, a single maze was created for the agent to explore.
However, placing the agent in a different maze with the exact same setup will perform signficantly worse, as the agent would not understand that where there might be walls in one maze there are now paths the agent needs to take.

Similarly, this may be the case of the experiment training method.
First, the agent is given a sub-space of the large environment.
That is, the agent is only given states closer to the goals.
The agent then might over-fit on those states, which may initially make one direction significantly more appealing than the rest.
Therefore, the weights associated with that move are increasingly higher than the other actions.
Then, as the model is brought out to new states in the environment, it may still believe that it's in that smaller sub-space of the whole environment.
The issue is that the actions preferred within the sub-space have significantly higher weights than the other actions, making it hard to change the policy for only one state without changing the rest of the states.
This challenges the idea that the model would converge faster to the optimal strategy when shown the goal first.
When the model overfits near the goal, it has a significantly harder time working with the rest of the environment.
However, the question still remains on the root cause of this observation, since this overfitting seems to be affecting both the experiemntal method and the control method, which doesn't have the same explanation for its overfitting.
It could be several factors, ranging from the design of the neural network itself, to the configuration of learning rates and training sizes.
Regardless, it brings up a question about how to navigate the overfitting problem with an agent starting within a smaller sub-space of the environment.

There are some potential solutions that could be implemented to navigate this potential issue.
In Deep Q-Learning, the goal of the Q-Network is to approximate the function that converts state-action pairs to expected utilities \cite{article_reinforcement_learning_survey}.
Intermittently, the target network is updated with the weights of the Q-Network.
The goal of the target network is to store the expected utilities of looking one more step ahead.
When the target network is updated, the weights and biases in the Q-Network don't matter as much, since they need to be trained against a new target.
Therefore, we can modify the network to something of our choosing right after updating the target network.
This could either be by squishing all of the weights and biases to be within a given range, or by providing a completely new randomly generated network.
However, this adds some randomness to the training method, so there is a chance that the agent takes steps backwards during training.



% Overview of the Results
% - Discussing how the model liked to pick one particular move
% - That particular move tended to switch up between one of two moves
% 
% Interpretation of the results
% - Model must have had high expected utilities for the move that got the agent to the solution in the beginning
% - Even when the target updated, the model still believed that taking that move would result in higher expected utility
% - 

% Bring up sources in introduction**

% 15-20 pages (not including title page or referenes)
% OH BOY
% Include code in project
% GitHub Link 


% For example, the experimental state is first fed states right next to the goal, making the replay database consist of just one state.
% This leads the model to learn that taking one particular action results in obtaining the max reward for being in the goal.
% However, as the model is given newer states slightly further from the goal, the replay network is still filled with a significant portion of that one state.
% As this continues, the expected utility of taking that action is stuck in a feedback loop that keeps increasing it.
% After some point, the replay is filled with another specific action that also, for some reason, has a high expected utility.












\newpage
\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}
